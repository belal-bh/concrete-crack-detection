# -*- coding: utf-8 -*-
"""Crack Detection - PyTorch Transfer Learning with Data Augmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mz_ArHYuTvuxngfwkltsZsZZ1RAcIVS8

# Concrete Crack Detection Using Transfer Larning

## Import Libraries
"""

import torch
import torch.nn as nn
import torchvision
from torchvision import datasets, transforms, models
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from datetime import datetime
import random
import sys, os
import shutil
from glob import glob
import imageio

"""## Load the Crack Dataset

- Link to the crack data set:  https://data.mendeley.com/datasets/5y9wdsg2zt/2
"""

# Data from: https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/5y9wdsg2zt-2.zip
!wget https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/5y9wdsg2zt-2.zip

!ls

!unzip -qq -o 5y9wdsg2zt-2.zip

!ls

!unrar x 'Concrete Crack Images for Classification.rar'
!ls

crack_images = os.listdir('Positive/')
print("Number of Crack Images: ", len(crack_images))

no_crack_images = os.listdir('Negative/')
print("Number of No Crack Images: ", len(no_crack_images))

# look at an image for fun
plt.imshow(imageio.imread('Positive/00001.jpg'))
plt.show()

## Visualize Random crack images
random_indices = np.random.randint(0, len(crack_images), size=4)
random_images = np.array(crack_images)[random_indices.astype(int)]

f, axarr = plt.subplots(2,2)
axarr[0,0].imshow(mpimg.imread(f'Positive/{random_images[0]}'))
axarr[0,1].imshow(mpimg.imread(f'Positive/{random_images[1]}'))
axarr[1,0].imshow(mpimg.imread(f'Positive/{random_images[2]}'))
axarr[1,1].imshow(mpimg.imread(f'Positive/{random_images[3]}'))

## Visualize Random noncrack images
random_indices = np.random.randint(0, len(no_crack_images), size=4)
random_images = np.array(no_crack_images)[random_indices.astype(int)]

f, axarr = plt.subplots(2,2)
axarr[0,0].imshow(mpimg.imread(f'Negative/{random_images[0]}'))
axarr[0,1].imshow(mpimg.imread(f'Negative/{random_images[1]}'))
axarr[1,0].imshow(mpimg.imread(f'Negative/{random_images[2]}'))
axarr[1,1].imshow(mpimg.imread(f'Negative/{random_images[3]}'))

"""## Train and Test Datasets"""

# remove data if already exist
!rm -r data
# Make directories to store the data Keras-style
!mkdir data
!mkdir data/train
!mkdir data/test
!mkdir data/train/noncrack
!mkdir data/train/crack
!mkdir data/test/noncrack
!mkdir data/test/crack

# Move the images
# Note: we will consider 'training' to be the train set
#       'validation' folder will be the test set
#       ignore the 'evaluation' set
!mv Negative/* data/train/noncrack
!mv Positive/* data/train/crack
# test set will be generated randomly from train set

crack_train = 'data/train/crack'
crack_test = 'data/test/crack/'
noncrack_train = 'data/train/noncrack/'
noncrack_test = 'data/test/noncrack/'

crack_files = os.listdir(crack_train)
noncrack_files = os.listdir(noncrack_train)

print(f"crack_files:{len(crack_files)}, noncrack_files:{len(noncrack_files)}")

for f in crack_files:
    if random.random() > 0.80:
        shutil.move(f'{crack_train}/{f}', crack_test)

for f in noncrack_files:
    if random.random() > 0.80:
        shutil.move(f'{noncrack_train}/{f}', noncrack_test)

# show the number of train and test images
print(f"Train set: crack={len(os.listdir(crack_train))}, noncrack={len(os.listdir(noncrack_train))}")
print(f"Test set: crack={len(os.listdir(crack_test))}, noncrack={len(os.listdir(noncrack_test))}")

# Note: normalize mmean and std are standardized for ImageNet
# https://github.com/pytorch/examples/blob/97304e232807082c2e7b54c597615dc0ad8f6173/imagenet
train_transform = transforms.Compose([
  transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),
  transforms.RandomRotation(degrees=15),
  transforms.ColorJitter(),
  transforms.CenterCrop(size=224),
  transforms.RandomHorizontalFlip(),
  transforms.ToTensor(),
  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])                
])

test_transform = transforms.Compose([
  transforms.Resize(size=256),
  transforms.CenterCrop(size=224),
  transforms.ToTensor(),
  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

train_dataset = datasets.ImageFolder(
    'data/train',
    transform=train_transform
)
test_dataset = datasets.ImageFolder(
    'data/test',
    transform=test_transform
)

# Data loader
# Usefull because it automatically generates batches in the training loop
# and takes care of shuffling

batch_size = 128
train_loader = torch.utils.data.DataLoader(
    dataset=train_dataset,
    batch_size=batch_size,
    shuffle=True
)

test_loader = torch.utils.data.DataLoader(
    dataset=test_dataset,
    batch_size=batch_size,
    shuffle=False
)

# Define the model
model = models.vgg16(pretrained=True)

# Freeze VGG weights
for param in model.parameters():
  param.requires_grad = False

print(model)

# We want to replace the 'classifier'
model.classifier

n_features = model.classifier[0].in_features
n_features

# We're doing binary classification
model.classifier = nn.Linear(n_features, 2)

# Let's see what the model is now
print(model)

# # Define the model
# class CNN(nn.Module):
#   def __init__(self, K):
#     super(CNN, self).__init__()

#     # define the conv layers
#     self.conv1 = nn.Sequential(
#         nn.Conv2d(3, 32, kernel_size=3, padding=1),
#         nn.ReLU(),
#         nn.BatchNorm2d(32),
#         nn.Conv2d(32, 32, kernel_size=3, padding=1),
#         nn.ReLU(),
#         nn.BatchNorm2d(32),
#         nn.MaxPool2d(2),
#     )

#     self.conv2 = nn.Sequential(
#         nn.Conv2d(32, 64, kernel_size=3, padding=1),
#         nn.ReLU(),
#         nn.BatchNorm2d(64),
#         nn.Conv2d(64, 64, kernel_size=3, padding=1),
#         nn.ReLU(),
#         nn.BatchNorm2d(64),
#         nn.MaxPool2d(2),
#     )

#     self.conv3 = nn.Sequential(
#         nn.Conv2d(64, 128, kernel_size=3, padding=1),
#         nn.ReLU(),
#         nn.BatchNorm2d(128),
#         nn.Conv2d(128, 128, kernel_size=3, padding=1),
#         nn.ReLU(),
#         nn.BatchNorm2d(128),
#         nn.MaxPool2d(2),
#     )

#     # Useful: https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool2d
#     # H_out = H_in + 2p - 2 --> p = 1 if H_out = H_in

#     # Easy to calculate output
#     # 32 > 16 >8 > 4

#     # define the linear layers
#     self.fc1 = nn.Linear(128 * 4 * 4, 1024)
#     self.fc2 = nn.Linear(1024, K)

#   def forward(self, x):
#     x = self.conv1(x)
#     x = self.conv2(x)
#     x = self.conv3(x)
#     x = x.view(x.size(0), -1)
#     x = F.dropout(x, p=0.5)
#     x = F.relu(self.fc1(x))
#     x = F.dropout(x, p=0.2)
#     x = self.fc2(x)
#     return x

# # Instantiate the model
# model = CNN(K)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
model.to(device)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())

# A function to encapsulate the training loop
def batch_gd(model, criterion, optimizer, train_loader, test_loader, epochs):

  # Stuff to store
  train_losses = np.zeros(epochs)
  test_losses = np.zeros(epochs)

  for it in range(epochs):
    t0 = datetime.now()
    train_loss = []
    for inputs, targets in train_loader:
      # move data to GPU
      inputs, targets = inputs.to(device), targets.to(device)

      # zero the parameter gradients
      optimizer.zero_grad()

      # Forward pass
      outputs = model(inputs)
      loss = criterion(outputs, targets)

      # Backward and optimize
      loss.backward()
      optimizer.step()

      train_loss.append(loss.item())

    # Get train loss and test loss
    train_loss = np.mean(train_loss) # a little misleading

    test_loss = []
    for inputs, targets in test_loader:
      inputs, targets = inputs.to(device), targets.to(device)
      outputs = model(inputs)
      loss = criterion(outputs, targets)
      test_loss.append(loss.item())
    test_loss = np.mean(test_loss)

    # Save losses
    train_losses[it] = train_loss
    test_losses[it] = test_loss

    dt = datetime.now() - t0
    print(f"Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Duration: {dt}")
  return train_losses, test_losses

train_losses, test_losses = batch_gd(
    model,
    criterion,
    optimizer,
    train_loader,
    test_loader,
    epochs=6
)

# Plot the train loss and test loss per iteration
plt.plot(train_losses, label='train loss')
plt.plot(test_losses, label='test loss')
plt.legend()
plt.show()

# Accuracy

n_correct = 0.
n_total = 0.
for inputs, targets in train_loader:
  # move data to GPU
  inputs, targets = inputs.to(device), targets.to(device)

  # Forward pass
  outputs = model(inputs)

  # Get prediction
  # torch.max returns both max and argmax
  _, predictions = torch.max(outputs, 1)

  # update counts
  n_correct += (predictions == targets).sum().item()
  n_total += targets.shape[0]

train_acc = n_correct / n_total

n_correct = 0.
n_total = 0.
for inputs, targets in test_loader:
  # move data to GPU
  inputs, targets = inputs.to(device), targets.to(device)

  # Forward pass
  outputs = model(inputs)

  # Get prediction
  # torch.max returns both max and argmax
  _, predictions = torch.max(outputs, 1)

  # update counts
  n_correct += (predictions == targets).sum().item()
  n_total += targets.shape[0]

test_acc = n_correct / n_total
print(f"Train acc: {train_acc:.4f}. Test acc: {test_acc:.4f}")

"""# Save and Load Model"""

# Look at the state dict
model.state_dict()

# Save the model
saved_model_file = 'crack_detection_transfer_learning.pt'
torch.save(model.state_dict(), saved_model_file)

!ls

# Load the model
# Note: this makes more sense and is more compact when
# your model is a big class, as we will be seeing later.

# Define the model
model2 = models.vgg16(pretrained=True)

# Freeze VGG weights
for param in model2.parameters():
  param.requires_grad = False


# We want to replace the 'classifier'
model2.classifier

n_features = model2.classifier[0].in_features
n_features

# We're doing binary classification
model2.classifier = nn.Linear(n_features, 2)

# Let's see what the model is now
print(model2)
model2.load_state_dict(torch.load(saved_model_file))

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
model2.to(device)

# Evaluate the new model
# Resuls should be the same!

n_correct = 0.
n_total = 0.
for inputs, targets in train_loader:
  # move data to GPU
  inputs, targets = inputs.to(device), targets.to(device)

  # Forward pass
  outputs = model2(inputs)

  # Get prediction
  # torch.max returns both max and argmax
  _, predictions = torch.max(outputs, 1)

  # update counts
  n_correct += (predictions == targets).sum().item()
  n_total += targets.shape[0]

train_acc = n_correct / n_total

n_correct = 0.
n_total = 0.
for inputs, targets in test_loader:
  # move data to GPU
  inputs, targets = inputs.to(device), targets.to(device)

  # Forward pass
  outputs = model2(inputs)

  # Get prediction
  # torch.max returns both max and argmax
  _, predictions = torch.max(outputs, 1)

  # update counts
  n_correct += (predictions == targets).sum().item()
  n_total += targets.shape[0]

test_acc = n_correct / n_total
print(f"Train acc: {train_acc:.4f}. Test acc: {test_acc:.4f}")

# Download the model
from google.colab import files
files.download(saved_model_file)

"""# Related Work

- [Concrete-Crack-Detection](https://github.com/konskyrt/Concrete-Crack-Detection) 
The model acheived 98% accuracy on the validation set. (Same dataset)
"""